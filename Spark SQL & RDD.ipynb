{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb33ff2-6b15-4877-96e3-ac84133b8fbd",
   "metadata": {},
   "source": [
    "# Lab 03 - Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59919a-27b2-4173-a489-e5992fedb413",
   "metadata": {},
   "source": [
    "### Solving with RDD data structure\n",
    "#### 1. Input Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53976d88-4fda-4cdd-b0fa-0dc4f4bd96a7",
   "metadata": {},
   "source": [
    "#### 1.1 register.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837d0290-0eab-4790-9e8c-d2623c6f8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath01 = \"/data/students/bigdata_internet/lab3/register.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820a3c46-b1da-42db-956a-e4852c582dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file\n",
    "register = sc.textFile(inputpath01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76bce44f-9eb6-48fa-9331-bacb961696b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data\n",
    "register_data = register.map(lambda line: line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760e704-0db9-404c-b098-99680e5469c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_header = register_data.first()\n",
    "new_rdd = register_data.filter(lambda x: x!= register_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9b5d8-814d-4eb3-bcc0-b223f6e47008",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "##### 1.1.1 How many rows of data we obtain before and after the data cleaning above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b113bb59-228d-4377-a2de-f6a606226aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total rows before cleaning data is: 25319028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of rows before data cleaning\n",
    "num_rows_before = new_rdd.count()\n",
    "print (\"The total rows before cleaning data is:\", num_rows_before) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97ae2da-34db-44ea-84f2-14e725cfa209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======================================>                   (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total rows after cleaning data is: 19337258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Clean rows where used_slots = 0 or free_slots = 0\n",
    "cleanRowsRDD = new_rdd.filter((lambda record: record[2] != \"0\" and record[3] != \"0\"))\n",
    "\n",
    "claen_data = cleanRowsRDD.count()\n",
    "print (\"The total rows after cleaning data is:\", claen_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c7f8ca6-c499-4b69-b677-3bf1fa91098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19337258 rows with value of '0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Task 2: Get rows where used_slots = 0 or free_slots = 0\n",
    "RowsRDD = new_rdd.filter(lambda record: record[2] == \"0\" and record[3] == \"0\")\n",
    "\n",
    "with0 = RowsRDD.count()\n",
    "print (\"There are\", claen_data, \"rows with value of '0'\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f236170-9a96-4c40-af6f-b03c5ebb9bad",
   "metadata": {},
   "source": [
    "#### 1.2 stations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b68f3da-65ec-400e-b5c6-6aee33131fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath02 = \"/data/students/bigdata_internet/lab3/stations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41f5bef-cb3b-420e-9ae8-f3e9190fb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file\n",
    "stations = sc.textFile(inputpath02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf35a375-0efe-4ede-b3dd-89fa49b11012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data\n",
    "stations_data = stations.map(lambda line: line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc79fd-4ffd-47f2-a8bb-bbfec8c54033",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = stations_data.first()\n",
    "stationsRDD= stations_data.filter(lambda x: x!= header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac0cb0-f792-41a7-b729-f4b5319b2017",
   "metadata": {},
   "source": [
    "### 2. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6988bf8-1cdd-4823-b8dd-5a2bb7c42a65",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "##### 2.1 Computes the criticality value C(Si, Tj) for each pair C(Si, Tj)\n",
    "##### 2.2 Selects only the critical pairs C(Si, Tj) having a criticality value C(Si, Tj) greater than a minimum threshold. The minimum criticality threshold is a float between 0 and 1 passed as an argument of the application.\n",
    "##### 2.3. Order the results by increasing criticality. If there are two or more records characterized by the same criticality value, consider the station id value (in ascending order). If also the station is the same, consider the day of the week (ascending from Monday to Sunday) and finally the hour (ascending from 0 to 23)\n",
    "##### 2.4. Store the sorted critical pairs  C(Si, Tj) in the output folder (also an argument of the application), by using a csv files (with header), where columns are separated by \"tab\". \n",
    "##### 2.5 How many critical pairs do you obtain? Report also the complete output result of the applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e88161f-5431-42df-a62e-e7af0d1e8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting weekday and hour from the timestamp\n",
    "\n",
    "from datetime import datetime\n",
    "dayHourRDD = new_rdd.map(lambda x: (x[0],(datetime.strftime(datetime.strptime(x[1], \"%Y-%m-%d %H:%M:%S\"),\"%A %H\").split(\" \")[0],\n",
    "                                          datetime.strftime(datetime.strptime(x[1], \"%Y-%m-%d %H:%M:%S\"),\"%A %H\").split(\" \")[1]),x[2],x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a68602d-bed9-4835-9351-588e45919d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedRDD = dayHourRDD.groupBy(lambda x: (x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60408a59-ab69-46b1-9800-d5cdb2a904ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to a list\n",
    "resultRDD = groupedRDD.map(lambda x: (x[0], list(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a5352b3-b6e1-4cb4-8503-abd5a280a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conuting the number of \"0\" slots in each group to calculate the criticality\n",
    "def zero_counter(items):\n",
    "    total = 0;\n",
    "    for i in items:\n",
    "        if i[3] == '0':\n",
    "            total+=1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c29b4e-81d8-46fd-8427-122ea627077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newRDD = resultRDD.map(lambda x: (x[0],x[1],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e57c6e-a2ea-4418-b5f4-78976209bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to get the zeros of each group\n",
    "zeroSlots = newRDD.map(lambda x: ( x[0], x[1], x[2], zero_counter(x[1]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15ef00af-298b-487f-a191-de9d06e9a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the criticality\n",
    "criticality = zeroSlots.map(lambda x: ( x[0], x[1], x[2], x[3], x[3]/x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83c9fe6d-d0bd-43b3-abbe-cb6d0b0e1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import sys\n",
    "# Define the minimum criticality threshold (replace 0.5 with your desired threshold)\n",
    "min_threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48452080-3354-4304-a2f5-0d16fb891afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdRDD = criticality.filter(lambda x: x[4]>min_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b89d152-8b25-445c-9781-cdf42eff32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = { 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5,'Saturday': 6, 'Sunday': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b8c9f03-1348-43ba-ab37-b1e985901d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sortedRDD = thresholdRDD.sortBy(lambda x: (x[4], x[0][0], weekdays.get(x[0][1][0]),x[0][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b188ba9d-95e6-4aed-892c-698cf2ab2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('stationid', ('weekday', 'hour', criticality))\n",
    "organizedRDD = sortedRDD.map(lambda x: (x[0][0],(x[1][0][1][0],x[1][0][1][1], x[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dd2f1b1-1d9b-44e7-909f-9fb978a9f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('221', ('Friday', '10', 0.6010016694490818)),\n",
       " ('221', ('Friday', '09', 0.6016666666666667)),\n",
       " ('221', ('Friday', '20', 0.6016666666666667))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organizedRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25297066-cd41-49e2-8616-44579d34cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRDD = stationsRDD.map(lambda x : (x[0],(x[1],x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b1fe1c1-e259-4b15-aa44-9e9be143357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', ('2.180019', '41.397978')),\n",
       " ('2', ('2.176414', '41.394381')),\n",
       " ('3', ('2.181164', '41.393750'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64bf4b7a-0625-49e8-ac4c-84d44dbff611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('stationid', (('weekday', 'hour', criticality), ('long', 'lot')))\n",
    "joinedRDD = organizedRDD.join(pairRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "451b4ae2-83a5-4577-90c4-112ae4de739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#and each row looks like this ('10', '2.185206', '41.384875', 'Saturday', '00', 0.622107969151671)\n",
    "resultRDD = joinedRDD.map(lambda x: (x[0],x[1][1][0],x[1][1][1],x[1][0][0],x[1][0][1], x[1][0][2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8865f22a-674e-4fd7-9033-995c70ffa868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('10', '2.185206', '41.384875', 'Saturday', '00', 0.6230769230769231),\n",
       " ('221', '2.160747', '41.401806', 'Friday', '10', 0.6010016694490818),\n",
       " ('221', '2.160747', '41.401806', 'Friday', '09', 0.6016666666666667)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final result\n",
    "resultRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaf051f4-110d-4848-b2b8-bd6fb5d156a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the header\n",
    "header = (\"Station\", \"Lattitude\", \"Longtitude\", \"Weekday\", \"Hour\", \"Criticality\")\n",
    "headerRDD = sc.parallelize([header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67acd825-2e03-4aa4-8fca-f2f71d9335cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we make a union to mix two rdds\n",
    "ResultRDD= headerRDD.union(resultRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3006e695-6d76-4b13-abbc-3501992c6bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "saveRDD = ResultRDD.map(lambda x: str(x[0])+'\\t'+str(x[1])+'\\t'+str(x[2])+'\\t'+str(x[3])+'\\t'+str(x[4])+'\\t'+str(x[5]))\n",
    "#saving  the final result as csv file\n",
    "saveRDD.coalesce(1).saveAsTextFile(\"lab3_s302866_RDDresult.csv/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2091efdb-6c76-4407-8733-32a8e472eaca",
   "metadata": {},
   "source": [
    "#     ********************************************************************\n",
    "#     ********************************************************************\n",
    "#     ********************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e129b4-12ec-4c14-8175-a5dd9a89a0b7",
   "metadata": {},
   "source": [
    "# Solving with SQL\n",
    "#### 1. Input Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ea1bb-e030-4303-ba6b-6d9482abea0e",
   "metadata": {},
   "source": [
    "#### 1.1 register.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ec411ca-d081-4c13-b5fc-ca1be2bf6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath01 = \"/data/students/bigdata_internet/lab3/register.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5096842-ed58-49ce-ac9b-c9f1c0e1e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the content of the input file and store it into a DataFrame\n",
    "dfReadings=spark.read.option(\"sep\", \"\\t\").load(inputpath01, format = \"csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edb32ab0-1f68-4677-8a64-b69c80fe59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the “table name” readings to dfReadings\n",
    "dfReadings.createOrReplaceTempView(\"registerSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969dbc1e-ea0c-4ff6-972f-ee5fd4ec3f3f",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "##### 1.1.1 How many rows of data we obtain before and after the data cleaning above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6384ea52-986b-4971-ba21-26c1c42c1f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total rows before cleaning data is: 25319028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of rows before data cleaning\n",
    "num_rows_before = dfReadings.count()\n",
    "print (\"The total rows before cleaning data is:\", num_rows_before) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44a400b2-70f0-499e-a8a9-9815addbebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean rows where used_slots = 0 or free_slots = 0\n",
    "cleanRows=spark.sql(\"SELECT used_slots, free_slots FROM registerSQL WHERE used_slots != 0 AND free_slots != 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "febf8351-59ed-426b-a690-38fb90cffe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19337258"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanRows.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa697d6-a58c-4107-b610-b2054c7dca95",
   "metadata": {},
   "source": [
    "#### 1.2 stations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a62f37eb-ab4f-4561-8121-67230bf61a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath02 = \"/data/students/bigdata_internet/lab3/stations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0aedad0-afc7-4f52-93bd-7a0316c35552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the content of the input file\n",
    "dfStation = spark.read.option(\"sep\", \"\\t\").load(inputpath02, format = \"csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49ddfa83-fde2-4b48-b9ea-0bba76f40300",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStation.createOrReplaceTempView(\"station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be518f0-97d8-4884-ba59-89d5aec97a6d",
   "metadata": {},
   "source": [
    "### 2. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17f9d4-5514-4117-86ed-9c6acd8e5859",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "##### 2.1 Computes the criticality value C(Si, Tj) for each pair C(Si, Tj)\n",
    "##### 2.2 Selects only the critical pairs C(Si, Tj) having a criticality value C(Si, Tj) greater than a minimum threshold. The minimum criticality threshold is a float between 0 and 1 passed as an argument of the application.\n",
    "##### 2.3. Order the results by increasing criticality. If there are two or more records characterized by the same criticality value, consider the station id value (in ascending order). If also the station is the same, consider the day of the week (ascending from Monday to Sunday) and finally the hour (ascending from 0 to 23)\n",
    "##### 2.4. Store the sorted critical pairs  C(Si, Tj) in the output folder (also an argument of the application), by using a csv files (with header), where columns are separated by \"tab\". \n",
    "##### 2.5 How many critical pairs do you obtain? Report also the complete output result of the applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46b65946-b4c0-4853-a182-d41f34c3d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, date_format, col\n",
    "\n",
    "# Extract day of the week and hour from the timestamp\n",
    "dfReadings = dfReadings.withColumn(\"day_of_week\", date_format(\"timestamp\", \"EEEE\"))\n",
    "dfReadings = dfReadings.withColumn(\"hour\", hour(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e4110cf-2fe0-42ba-99e3-5bfad080ef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+----------+-----------+----+\n",
      "|station|          timestamp|used_slots|free_slots|day_of_week|hour|\n",
      "+-------+-------------------+----------+----------+-----------+----+\n",
      "|      1|2008-05-15 10:01:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:02:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:04:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:06:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:08:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:10:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:12:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:14:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:16:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:18:00|         0|        18|   Thursday|  10|\n",
      "|      1|2008-05-15 10:20:00|         2|        16|   Thursday|  10|\n",
      "|      1|2008-05-15 10:22:00|         3|        15|   Thursday|  10|\n",
      "|      1|2008-05-15 10:24:00|         3|        15|   Thursday|  10|\n",
      "|      1|2008-05-15 10:26:00|         3|        15|   Thursday|  10|\n",
      "|      1|2008-05-15 10:28:00|         4|        14|   Thursday|  10|\n",
      "|      1|2008-05-15 10:30:00|         0|        12|   Thursday|  10|\n",
      "|      1|2008-05-15 10:32:00|         4|        14|   Thursday|  10|\n",
      "|      1|2008-05-15 10:34:00|         4|        14|   Thursday|  10|\n",
      "|      1|2008-05-15 10:36:00|         4|        14|   Thursday|  10|\n",
      "|      1|2008-05-15 10:38:00|         4|        14|   Thursday|  10|\n",
      "+-------+-------------------+----------+----------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfReadings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2788f57c-9c41-4bb2-9408-e6e229f19193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- used_slots: integer (nullable = true)\n",
      " |-- free_slots: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfReadings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52316379-a5f2-4644-9977-73637ee725e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "zeroSlotsDF = dfReadings.withColumn(\"zeros\", func.when(cleanRows[\"free_slots\"] == 0, 1).otherwise(0))\n",
    "zeroSlotsDF.createOrReplaceTempView(\"zeroRegister\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cad865b-2276-4757-94bf-ea96c0ad974b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:======================================>                   (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----+--------------------+\n",
      "|station|day_of_week|hour|         criticality|\n",
      "+-------+-----------+----+--------------------+\n",
      "|    263|  Wednesday|  23|  0.1634349030470914|\n",
      "|    264|   Thursday|  20|0.003333333333333...|\n",
      "|    265|     Friday|  13|                 0.0|\n",
      "|    266|     Friday|  10|0.001666666666666...|\n",
      "|    266|  Wednesday|  10|0.001754385964912...|\n",
      "|    268|   Thursday|  21| 0.05175292153589316|\n",
      "|    269|     Sunday|   5|                 0.0|\n",
      "|    270|   Saturday|  15|                 0.0|\n",
      "|    270|     Sunday|  11|                 0.0|\n",
      "|    270|   Thursday|   8|                 0.0|\n",
      "|    272|   Saturday|  11|0.001760563380281...|\n",
      "|    273|     Sunday|  15|0.001754385964912...|\n",
      "|    273|    Tuesday|  13|0.001675041876046...|\n",
      "|    277|     Sunday|   9| 0.10526315789473684|\n",
      "|    277|  Wednesday|  12|0.001814882032667...|\n",
      "|    277|     Monday|   1| 0.08333333333333333|\n",
      "|    278|     Sunday|  22| 0.22777777777777777|\n",
      "|    279|   Saturday|   5|0.001757469244288...|\n",
      "|    280|     Friday|  12|                 0.0|\n",
      "|    280|   Saturday|   4|                 0.0|\n",
      "+-------+-----------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "criticality = zeroSlotsDF.groupBy([\"station\", \"day_of_week\", \"hour\"]) \\\n",
    "       .agg(func.sum(\"zeros\").alias(\"sum_zeros\"), func.count(\"free_slots\").alias(\"count_free_slots\")) \\\n",
    "       .selectExpr(\"station\", \"day_of_week\", \"hour\", \"sum_zeros / count_free_slots as criticality\")\n",
    "\n",
    "criticality.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b34911e0-ee88-4bf8-aa20-55e62dc799ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum criticality is 0.6\n",
    "MinimumCriticality = 0.6\n",
    "# now we filter the groups which have criticality over the min threshold\n",
    "thresholdDF = criticality.select(\"*\").where(criticality[\"criticality\"] >= 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cf36db6-d3e9-49e6-870c-13c5d0d95b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----+------------------+\n",
      "|station|day_of_week|hour|       criticality|\n",
      "+-------+-----------+----+------------------+\n",
      "|     58|     Sunday|  23|             0.625|\n",
      "|    221|     Friday|  20|0.6143572621035058|\n",
      "|    221|     Sunday|  20|0.6748681898066784|\n",
      "|    221|   Saturday|   7|0.6210526315789474|\n",
      "|    221|     Sunday|  21|0.7095070422535211|\n",
      "|    221|     Sunday|  10|0.6203866432337434|\n",
      "|    221|   Thursday|  17|0.6494156928213689|\n",
      "|    221|     Monday|   3|0.6256590509666081|\n",
      "|    221|     Sunday|  13|0.6731107205623902|\n",
      "|    221|   Saturday|   6|0.6239015817223199|\n",
      "|    221|     Sunday|  15|0.6771929824561403|\n",
      "|    221|   Thursday|  13|0.6541737649063032|\n",
      "|    221|   Thursday|  18|0.6444073455759599|\n",
      "|     10|     Friday|  22|0.6230769230769231|\n",
      "|    221|     Sunday|  11|0.6214788732394366|\n",
      "|    221|    Tuesday|   3|0.6263157894736842|\n",
      "|    221|     Sunday|   4|0.6298245614035087|\n",
      "|    221|   Saturday|   8|0.6210526315789474|\n",
      "|    221|     Sunday|   8|0.6256590509666081|\n",
      "|    221|    Tuesday|   5|0.6263345195729537|\n",
      "+-------+-----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thresholdDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74ce481a-1528-451e-bac3-97fbb57e3a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- criticality: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thresholdDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3929aac-2fa4-4b07-8945-ff3701b66a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStation.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "503a3cdd-6f36-4426-94b1-07855367d0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'longitude', 'latitude', 'name']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfStation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e55d15c3-7e93-4c93-972a-8e200bbe9cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['station', 'day_of_week', 'hour', 'criticality']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholdDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8eb1ed00-6019-4bcd-95ce-34b3e0df1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the join using the correct column names\n",
    "joinedDfs = thresholdDF.join(dfStation, thresholdDF.station == dfStation.id).select(\n",
    "    thresholdDF.station, dfStation.longitude, dfStation.latitude, \n",
    "    thresholdDF.day_of_week, thresholdDF.hour, thresholdDF.criticality\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31c01609-3fc9-45a3-ad14-62f8dc9b6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import *\n",
    "\n",
    "weekdays = { 'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5,'Saturday': 6, 'Sunday': 7}\n",
    "# spark.udf.register(\"weekday_order\", lambda x: weekdays.get(x), IntegerType()) //once worked then didn't\n",
    "weekday_order = udf(lambda x: weekdays.get(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "232b265f-6e0d-401b-bb09-65594535469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column named weekday_id and the data is generated using weekday_order UDF\n",
    "\n",
    "joined = joined_dfs.withColumn(\"weekday_id\", weekday_order(joined_dfs.day_of_week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c98d112-df4e-4f1a-ae39-8d33cb6afa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedDf = joined.sort(\"criticality\",\"station\",\"weekday_id\",\"hour\",ascending=[True,True,True,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba8ceff9-4f20-4857-aaa1-c97ff1229706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday_id column is not needed so we drop it\n",
    "final = sortedDf.drop(\"weekday_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba88ec47-2ece-4b79-be01-6ad1dd9be150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to add the header we create an rdd with the column names\n",
    "#obviously there are different way to do that\n",
    "header = (\"Station\", \"Lattitude\", \"Longtitude\", \"Weekday\", \"Hour\", \"Criticality\")\n",
    "header_rdd = sc.parallelize([header])\n",
    "\n",
    "#first we change df to rdd\n",
    "df_to_rdd = final.rdd\n",
    "\n",
    "#we make a union to mix two rdds\n",
    "result_rdd = header_rdd.union(df_to_rdd)\n",
    "\n",
    "\n",
    "#cast data to string then add \\t as seperator\n",
    "saveable_rdd = result_rdd.map(lambda x: str(x[0])+'\\t'+str(x[1])+'\\t'+str(x[2])+'\\t'+str(x[3])+'\\t'+str(x[4])+'\\t'+str(x[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e545c6bb-f6ff-41c5-8d31-6def050358bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#saving  the final result as csv file\n",
    "saveable_rdd.coalesce(1).saveAsTextFile(\"lab3_s308885_final_result.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2bcc47-9871-4467-b3e8-1a7bcb99a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
