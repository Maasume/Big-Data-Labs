{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5650a9e7-2153-4005-aafb-bb688985f7ec",
   "metadata": {},
   "source": [
    "## Laboratory 1 - Introduction to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4c02173-3050-4661-9b28-54ad02cb5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b058e550-7140-456a-b0c2-c81dd6faf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_rdd = rdd.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f6cef8a-af52-438c-b3b0-753d699409d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_rdd = fields_rdd.map(lambda l: int(l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2250378e-3393-4335-98d5-901eaa0bba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_sum = value_rdd.reduce(lambda v1, v2: v1+v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "953bd2bb-77de-4eb0-a3cd-6cd22e08ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is: 46\n"
     ]
    }
   ],
   "source": [
    "print(\"The sum is:\", value_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc3070-c5aa-4ac6-9889-6c04d3b212a2",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Which value is printed by the print statement?<Which is the purpose of each line of code?\n",
    "2. Where is the input file? On which file system?\n",
    "3. What happens if you open a Pyspark (YARN) notebook? Which is the difference in the Cluster manager\n",
    "interface at https://bigdatalab.polito.it? (https://bigdatalab.polito.it?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252eb00-05bc-4723-a669-e21b65af57e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Answers\n",
    "1. The value is going to be the sum of the second values in each line of the input file. In this case the result will be \"46\".<br>\n",
    "In the first line of code, using \"textfile\" method of SparkContext(sc), the RDD (Resilient Distributed Dataset) is created. The address of text file is specified.<br> In the second line to define \"fields_rdd\", the \"Map\" transformation is applied on the lines to split the elements. Also, each line of RDD, using the \"split\" method is divided into a list of values and a comma (,) is used as a separator.<br> As  all elements in each line are string, to calculate sum of values, it is necessary that the type of second element will be changed to integer type, therefore, using \"Map\" transformation they are converted from string to integer.<br> In the next step,to compute the sum of values, \"Reduce\" action is applied.In this code, the lambda function `lambda v1, v2: v1+v2` is employed for the purpose of conducting the addition operation.<br> At the end, using print, the sume of value, which is calculated in the previous step is shown.\n",
    "\n",
    "2. The path of input file (lab1_dataset.txt) is located in \"/data/students/bigdata_internet/lab1/\" that Spark cluster is configured.<br>\n",
    "The following command shows that the file system is an HDFS.<br>hdfs dfs -ls /data/students/bigdata_internet/lab1\n",
    "\n",
    "3. When a local PySpark is used, Spark runs on a single server.But with a YARN notebook, the Spark tasks are sent to a YARN cluster, which is mean access to the all clusters. So, the main difference when a PySpark notebook with YARN is used is related to how it manages resources. This makes it great for handling big data tasks because it can efficiently share the work across many machines.\n",
    "Local PySpark is faster than Yarn because initializing and its processing takes more time compared to the local PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee9607-95b1-400b-8275-581aadd9c28a",
   "metadata": {},
   "source": [
    "### 1.2 Execute in a pyspark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f848ad-623b-4e54-a712-40aad5d70fc8",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "1. What does --master local mean?<br>\n",
    "2. And what about --deploy-mode client ?<br>\n",
    "3. Where spark executors and driver executed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b37863-190f-48ed-a6f0-4e20cf6ec61b",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "--master local means that Spark runs on the local computer(a single server), not on a cluster of machines. It is used for testing and development on a single machine and its ressult is fast.<br>\n",
    "In --deploy-mode client submitter runs the driver outside of cluster. Also, the results can be saved on the local computer.<br>\n",
    "These workers or executors can run on a local machine or on other machines if there is accessebility to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fba14-0125-4aa3-a2ce-04eb192bef2e",
   "metadata": {},
   "source": [
    "### 1.3 Create a Spark script and run it from the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68078e5b-1db2-4753-a008-f3045922d6fb",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "In which file system are located your script and the\n",
    "/data/students/bigdata_internet/lab1/lab1_dataset.txt files?<br>\n",
    "Are they on the same file system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda96268-e872-423f-9f1f-59b0dc20c9d0",
   "metadata": {},
   "source": [
    "#### Answers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56eb55d-1072-4f95-bf26-9d846e8da095",
   "metadata": {},
   "source": [
    "The script is located on the local file system. And the \"lab1_dataset.txt\" files are located on the cluster.<br> No, they aren't the same the first one is a Spark script and the second one is a Hadoop file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91ee04-ef63-4fa1-9112-6395bdcddd40",
   "metadata": {},
   "source": [
    "### 2. Play with HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cc8e2-b91e-46bd-b9ed-634e0dc7de75",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "If you modify the local file, does the modifications automatically affect also the HDFS file? Now create a file in the file system of the gateway and copy it to your home in HDFS.\n",
    "Which is the complete path of your file in HDFS? And on the gateway local file system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7bf618-2781-486c-ab54-da6c01459a86",
   "metadata": {},
   "source": [
    "No, Altering a local file doesn't automatically impact the associated HDFS file. To change upload or copy the modified file is needed.<br> \n",
    "The complete path of HDFS: hdfs dfs -ls /user/s302866/ForLab01.txt<br> \n",
    "The local file system: echo $(pwd)/ForLab01.txt<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394ddc5-2e5c-47b0-ab27-4d880553f9e0",
   "metadata": {},
   "source": [
    "### 3. Run a Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ccc97-50b8-4027-8cdd-4af91b9f6023",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "Report the code you have written, and explain the goal of each instruction.<br>\n",
    "How does the output folder on HDFS look like? Why do you find multiple parts file in the folder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d532065-ee85-4368-821b-0f5069af7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"My app\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# Defining input and output path\n",
    "inputPath = \"/data/students/bigdata_internet/lab1/lab1_dataset.txt\"\n",
    "outputPath = \"lab1_result/\"\n",
    "\n",
    "# Creating RDD\n",
    "rdd = sc.textFile(inputPath)\n",
    "RDDLines = rdd.map(lambda line: line.split(\",\"))\n",
    "RDDValues = RDDLines.map(lambda l: (l[0],int(l[1])))\n",
    "new_rdd =  RDDValues.reduceByKey(lambda v1,v2: v1+v2)\n",
    "new_rdd.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94973dd9-1331-4893-a916-9d9bf543d270",
   "metadata": {},
   "source": [
    "from pyspark import SparkConf, SparkContext: Importing necessary modules from PySpark for configuring and using Spark.<br>\n",
    "\n",
    "conf = SparkConf().setAppName(\"My app\"): Creating a SparkConf object to set configuration parameters for the Spark application, and setting the application name to \"My app.\"<br>\n",
    "\n",
    "sc = SparkContext(conf=conf): Creating a SparkContext object with the specified configuration. SparkContext is the entry point for interacting with Spark functionality.<br>\n",
    "\n",
    "inputPath and outputPath: Specifies the path to the input text file on HDFS and where the result will be saved on HDFS.<br>\n",
    "\n",
    "rdd: Reads the content of the input file into an RDD, where each element represents a line in the file.<br>\n",
    "\n",
    "RDDLines: Splits each line into a list of fields by using a comma (,) as a delimiter.<br>\n",
    "\n",
    "RDDValues: Maps each list to a tuple,using map(), where the first element is used as the key, and the second element first is converted to an integer and then used as the value.<br>\n",
    "\n",
    "new_rdd: Reduces by key, summing up the values for each key (grouping by the first element of the tuple).<br>\n",
    "\n",
    "saveAsTextFile: Saves the final RDD as a text file in the specified output path on HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070a5fe-4d17-4300-a7a1-9b77354a1905",
   "metadata": {},
   "source": [
    "**** Using !spark-submit --master local --deploy-mode client 'lLab01Script.py'\n",
    "\n",
    "- SUCCESS: This file is empty and shows that the process is completed successfully without errors.\n",
    "\n",
    "- 00000: Is the output file with result <u>('alice', 14), ('bob', 11), ('john', 21)</u>. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a2075-9ab8-467d-a442-733119843d3e",
   "metadata": {},
   "source": [
    "When we save the RDD using saveAsTextFile, Spark writes each partition as a separate part file, resulting in multiple part files within the specified output directory. This organization helps maintain parallelism and scalability in distributed processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc508f0-9714-49f1-9398-f518bdc165ef",
   "metadata": {},
   "source": [
    "### 4. Bonus Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca21df0-3175-416f-b652-9c7526946288",
   "metadata": {},
   "source": [
    "#### Question: Report the code you have written, and explain the goal of each instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e1eb7-3c5a-49fb-a298-3c8e0764a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"My app\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# Defining input and output path\n",
    "inputPath = \"/data/students/bigdata_internet/lab1/lab1_dataset.txt\"\n",
    "outputPath = \"lab1_part4_solution/\"\n",
    "\n",
    "# Creating RDD\n",
    "rdd = sc.textFile(inputPath)\n",
    "RDDLines = rdd.map(lambda line: line.split(\",\"))\n",
    "RDDValues = RDDLines.map(lambda l: (l[0],int(l[1])))\n",
    "new_rdd =  RDDValues.reduceByKey(lambda v1,v2: v1+'-'+v2)\n",
    "new_rdd.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d08d3-4424-46e9-ad76-102c2f4e4c16",
   "metadata": {},
   "source": [
    "from pyspark import SparkConf, SparkContext: Importing necessary modules from PySpark for configuring and using Spark.<br>\n",
    "\n",
    "conf = SparkConf().setAppName(\"My app\"): Creating a SparkConf object to set configuration parameters for the Spark application, and setting the application name to \"My app.\"<br>\n",
    "\n",
    "sc = SparkContext(conf=conf): Creating a SparkContext object with the specified configuration. SparkContext is the entry point for interacting with Spark functionality.<br>\n",
    "\n",
    "inputPath and outputPath: Specifies the path to the input text file on HDFS and where the result will be saved on HDFS.<br>\n",
    "\n",
    "rdd: Reads the content of the input file into an RDD, where each element represents a line in the file.<br>\n",
    "\n",
    "RDDLines: Splits each line into a list of fields by using a comma (,) as a delimiter.<br>\n",
    "\n",
    "RDDValues: Maps each list to a tuple,using map(), where the first element is used as the key, and the second element first is converted to an integer and then used as the value.<br>\n",
    "\n",
    "new_rdd = RDDValues.reduceByKey(lambda v1, v2: v1 + '-' + v2): Reducing the key-value pairs by key (name) and concatenating the corresponding values with a hyphen (\"-\").<br>\n",
    "\n",
    "new_rdd.saveAsTextFile(outputPath): Saving the resulting RDD as text files in the specified output directory on HDFS. Each part file will contain the concatenated values for the corresponding key.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4304a0-862f-4b47-bfb6-839c928035b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
